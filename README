This article is to combine 2 styles onto a selected content image. We develop our work based on the method proposed by Gatys et al. (2015). To be specific, we use selected layers in the pre-trained 16 layers of VGG Network on the ImageNet dataset to train our content image, and style images respectively. Later, we calculate each given layer’s loss function of content image and style images. We also add a regularization which is calculated by total variation to balance the output graph’s pixels for a better visualization. Then, we sum the loss functions and regularization up by assigning different weights to them. The first type of outputs are the texture synthesis, which corresponds to the extraction of each style. By the texture synthesis created, we find out that the method proposed by Gatys et al. (2015) only work well on colorful types of images. If the contradictions of pixels are not large enough in a style graph, the exaction of style can not achieve a good result. This finding is shown clearly after we combine each style with the content features which are extracted from content image where the style lacking of contradiction is hard to distinguish on the combination output graph. In the end, we output our final combination with 2 styles together on a content feature "canvas" by selecting the styles which are colorful by pursuing better visualization.
